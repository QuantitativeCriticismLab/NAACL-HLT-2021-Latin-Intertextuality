{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook creates a word2vec model using the Bamman 2012 corpus lemmatized with the CLTK BackoffLatinLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import html\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "import collections\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NLP tools\n",
    "\n",
    "replacer = JVReplacer()\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "lemmatizer = BackoffLatinLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "def preprocess(text):\n",
    "        \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper iterator class to process raw text and to handle file by file. Avoids memory issues. \n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    \n",
    "    def __iter__(self):\n",
    "        tokenizer = TokenizeSentence('latin')\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            with open(os.path.join(self.dirname, fname), encoding='utf-8') as file:\n",
    "                sents = tokenizer.tokenize_sentences(file.read().replace('\\n', ''))\n",
    "                sents = [[token[1] for token in lemmatizer.lemmatize(preprocess(sent).split())] for sent in sents]\n",
    "                for sent in sents:\n",
    "                    yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Build Latin word2vec on Bamman data\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "latin_w2v_model = Word2Vec(MySentences(\"../models/data/latin_txt\"), size = 300, min_count=100, workers=cores-1, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_w2v_model.save(\"../models/latin_w2v_bamman_lemma300_100_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}