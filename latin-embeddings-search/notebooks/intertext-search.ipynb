{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "from itertools import combinations, product\n",
    "from functools import lru_cache\n",
    "from statistics import mean\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import Levenshtein\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "\n",
    "from pprint import pprint\n",
    "import urllib\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "TEXTS_FOLDER = \"../data/texts/process\"\n",
    "\n",
    "files = natsorted(glob.glob(f'{TEXTS_FOLDER}/*.tess'))\n",
    "MODEL = '../models/latin_w2v_bamman_lemma300_100_1'\n",
    "VECTORS = Word2Vec.load(MODEL).wv\n",
    "lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NLP tools\n",
    "\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comparison dataset\n",
    "\n",
    "comps = pd.read_csv('../data/datasets/comps.csv', index_col=0)\n",
    "comps['query_lemma'] = comps['query_lemma'].apply(lambda x: x.split())\n",
    "comps['target_lemma'] = comps['target_lemma'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text functions\n",
    "\n",
    "def preprocess(text):\n",
    "    replacer = JVReplacer()    \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    text = text.replace('ego---sed', 'egosed') # handle a tesserae text issue\n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = text.replace('â\\x80\\x94', ' ')\n",
    "    \n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def index_tess(text):\n",
    "    \n",
    "    textlines = text.strip().split('\\n')\n",
    "    # https://stackoverflow.com/a/61436083\n",
    "    def splitkeep(s, delimiter):\n",
    "        split = s.split(delimiter)\n",
    "        return [substr + delimiter for substr in split[:-1]] + [split[-1]]\n",
    "    textlines = [splitkeep(line, '>') for line in textlines if line]\n",
    "    return dict(textlines)\n",
    "\n",
    "def pp_tess(tess_dict):\n",
    "    return {k: preprocess(v) for k, v in tess_dict.items()}\n",
    "\n",
    "def text_lemmatize(lemma_pairs):\n",
    "    return \" \".join([lemma for _, lemma in lemma_pairs])\n",
    "\n",
    "def lem_tess(tess_dict):\n",
    "    return {k: text_lemmatize(lemmatizer.lemmatize(v.split())) for k, v in tess_dict.items()}\n",
    "\n",
    "def make_tess_file(str):\n",
    "    str = str.lower()\n",
    "    str = str.replace('lucan', 'lucan bellum_civile')\n",
    "    str = str.split('.')[0]\n",
    "    str = str.replace(' ', '.', 1)\n",
    "    str = str.replace(' ', '.part.', 1)\n",
    "    str += '.tess'\n",
    "    return str\n",
    "\n",
    "def make_tess_index(str):\n",
    "    str = str.replace('Lucan', 'luc.').replace('Ovid', 'ov.').replace('Statius', 'stat.').replace('Vergil', 'verg.')\n",
    "    str = str.replace('Metamorphoses', 'met.').replace('Thebaid', 'theb.').replace('Aeneid', 'aen.')\n",
    "    str = str.split('-')[0]\n",
    "    str = f'<{str}>'\n",
    "    return str\n",
    "\n",
    "def get_next_tess_index(index, n):\n",
    "    index = index.replace('>','')\n",
    "    index_base = index.split()[:-1]\n",
    "    index_ref = index.split()[-1]\n",
    "    index_ref_parts = index_ref.split('.')\n",
    "    index_ref_next = int(index_ref_parts[1])+n\n",
    "    next_index = f'{\" \".join(index_base)} {index_ref_parts[0]}.{index_ref_next}>'\n",
    "    \n",
    "    exceptions = ['<luc. 1.419>', '<luc. 7.855>', '<luc. 7.856>', '<luc. 7.857>', '<luc. 7.858>', '<luc. 7.859>', '<luc. 7.860>', '<luc. 7.861>', '<luc. 7.862>', '<luc. 7.863>', '<luc. 7.864>', '<luc. 9.414>',\n",
    "                  '<ov. met. 4.769>', \n",
    "                  '<stat. theb. 6.184>', '<stat. theb. 6.227>', '<stat. theb. 6.228>', '<stat. theb. 6.229>', '<stat. theb. 6.230>', '<stat. theb. 6.231>', '<stat. theb. 6.232>', '<stat. theb. 6.233>', '<stat. theb. 9.760>']\n",
    "    if next_index in exceptions: # Handle missing data\n",
    "        return None\n",
    "    else:\n",
    "        return next_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ngrams\n",
    "\n",
    "def generate_ngrams(words_list, n):\n",
    "    # Cf. https://www.techcoil.com/blog/how-to-generate-n-grams-in-python-without-using-any-external-libraries/\n",
    "    ngrams_list = []\n",
    "\n",
    "    for num in range(0, len(words_list)):\n",
    "        ngram = ' '.join(words_list[num:num + n])\n",
    "        ngrams_list.append(ngram)\n",
    "    \n",
    "    ngrams_list = [item.split() for item in ngrams_list if len(item.split()) == n]\n",
    "    \n",
    "    return ngrams_list\n",
    "\n",
    "def generate_ngrams_interval(words_list, index, n, tess_dict, interval):\n",
    "    \n",
    "    limit = len(words_list) + interval + 1 # add one to avoid interval-based fencepost problem\n",
    "\n",
    "    while len(words_list) < limit:\n",
    "        words_extend = get_next_tess_index(index, 1)\n",
    "        if words_extend:\n",
    "            words_list += tess_dict[words_extend].split()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    words_list = words_list[:limit]\n",
    "    \n",
    "    ngrams_list = []\n",
    "\n",
    "    for num in range(0, len(words_list)):\n",
    "        ngram = ' '.join(words_list[num:num + n])\n",
    "        ngrams_list.append(ngram)\n",
    "    \n",
    "    ngrams_list = [item.split() for item in ngrams_list if len(item.split()) == n]\n",
    "    \n",
    "    return ngrams_list\n",
    "\n",
    "def ngram_tess(tess_dict, n=2, interval=0):\n",
    "    return {k: generate_ngrams_interval(v.split(), k, n, tess_dict, interval) for k, v in list(tess_dict.items())[:-1]} # Stop short of last item because of ngram lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity functions\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_similarities(terms, model):\n",
    "    sims = []\n",
    "    terms = list(terms)\n",
    "    terms_ = set([y for x in terms for y in x])\n",
    "    oov = [term for term in terms_ if term not in model.vocab]\n",
    "\n",
    "    for term in terms:\n",
    "        if term[0] in oov or term[1] in oov:\n",
    "            sim = -1\n",
    "        else:\n",
    "            sim = model.similarity(term[0], term[1])\n",
    "        sims.append(sim)\n",
    "\n",
    "    return sims\n",
    "\n",
    "# pair-aware mean\n",
    "def mean_similarities(terms, sims):\n",
    "    max_sim_index = sims.index(max(sims))\n",
    "    if max_sim_index == 0:\n",
    "        pair_index = 3\n",
    "    elif max_sim_index == 1:\n",
    "        pair_index = 2\n",
    "    elif max_sim_index == 2:\n",
    "        pair_index = 1\n",
    "    elif max_sim_index == 3:\n",
    "        pair_index = 0\n",
    "    return (sims[max_sim_index] + sims[pair_index])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment code to run full intertext search over texts\n",
    "\n",
    "# # Get intertext search results\n",
    "\n",
    "# results_ = []\n",
    "\n",
    "# for i, row in tqdm(comps.iterrows(), total=comps.shape[0]):    \n",
    "\n",
    "#     search_files = natsorted([file for file in files if row['intertext_author'] in file])\n",
    "    \n",
    "#     if np.isnan(row['interval']):\n",
    "#         interval = 0\n",
    "#     else:\n",
    "#         interval = int(row['interval'])\n",
    "    \n",
    "#     n = row['query_length'] + interval\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for file in search_files:\n",
    "#         with open(file, 'r') as f:\n",
    "#             contents = f.read()\n",
    "#             tess_dict = index_tess(contents)    \n",
    "#             tess_dict = pp_tess(tess_dict)\n",
    "#             if lemmatize:\n",
    "#                 tess_dict = lem_tess(tess_dict)\n",
    "#         tess_dict = ngram_tess(tess_dict, n, interval)           \n",
    "\n",
    "#         for item in list(tess_dict.items()):            \n",
    "#             index = item[0]\n",
    "#             ngrams = item[1]\n",
    "#             for ngram in ngrams:                \n",
    "#                 orderfree = row['orderfree']\n",
    "#                 if orderfree:\n",
    "#                     combs = list(combinations(ngram, 2))\n",
    "#                 else:\n",
    "#                     combs = [ngram]\n",
    "\n",
    "#                 for comb in combs:\n",
    "#                     pairs = tuple(product(row[\"query_lemma\"], comb))\n",
    "#                     dists = get_similarities(pairs, VECTORS)\n",
    "#                     dists_sum = mean_similarities(pairs, dists)\n",
    "#                     if dists_sum >= row[\"similarity\"]:\n",
    "#                         results.append((index, dists_sum, row['query_lemma'], list(comb)))\n",
    "#     results_.append((row['index'], results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp results saved at temp/results_naacl2021_search.p\n"
     ]
    }
   ],
   "source": [
    "# Create time-stamped file for results; cf. https://stackoverflow.com/a/14115286\n",
    "# output_path = f\"{os.path.join('temp', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))}-results.p\"\n",
    "# pickle.dump(results_, open(output_path, 'wb'))\n",
    "\n",
    "# For paper results, uncomment to download and run remaining cells\n",
    "# url = 'https://utexas.box.com/shared/static/2m2e09dijfxgqg3nnxei2cuttc7qu7kd.p'\n",
    "# urllib.request.urlretrieve (url, 'temp/results_naacl2021_search.p')\n",
    "\n",
    "# output_path = 'temp/results_naacl2021_search.p'\n",
    "# results_ = pickle.load(open(output_path, 'rb'))\n",
    "# print(f'Temp results saved at {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ranks\n",
    "\n",
    "ranks = [len(result[1]) for result in results_ if len(result[1]) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.3911290607923982\n",
      "\n",
      "Checking the following values for k [1, 3, 5, 10, 25, 50, 75, 100, 250]\n",
      "\n",
      "\tRecall at k=1: 0.2913135593220339\n",
      "\tPrecision at k=1: 0.2913135593220339\n",
      "\n",
      "\tRecall at k=3: 0.4385593220338983\n",
      "\tPrecision at k=3: 0.18809631985461153\n",
      "\n",
      "\tRecall at k=5: 0.5180084745762712\n",
      "\tPrecision at k=5: 0.15219421101774042\n",
      "\n",
      "\tRecall at k=10: 0.5953389830508474\n",
      "\tPrecision at k=10: 0.10575837410613474\n",
      "\n",
      "\tRecall at k=25: 0.673728813559322\n",
      "\tPrecision at k=25: 0.0608554205339202\n",
      "\n",
      "\tRecall at k=50: 0.7245762711864406\n",
      "\tPrecision at k=50: 0.038876889848812095\n",
      "\n",
      "\tRecall at k=75: 0.75\n",
      "\tPrecision at k=75: 0.029737903225806453\n",
      "\n",
      "\tRecall at k=100: 0.7711864406779662\n",
      "\tPrecision at k=100: 0.024760220393170534\n",
      "\n",
      "\tRecall at k=250: 0.8241525423728814\n",
      "\tPrecision at k=250: 0.013574107999651051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute recall & precision at k; computer MRR\n",
    "\n",
    "ks = [1, 3, 5, 10, 25, 50, 75, 100, 250]\n",
    "\n",
    "def recall_at_k(ranks, k):\n",
    "    n = len([rank for rank in ranks if rank <= k])\n",
    "    d = len(ranks)\n",
    "    recall = n/d\n",
    "    return recall\n",
    "\n",
    "def precision_at_k(ranks, k):\n",
    "    n = len([rank for rank in ranks if rank <= k])\n",
    "    d = sum([rank if rank<=k else k for rank in ranks])\n",
    "    precision = n/d\n",
    "    return precision\n",
    "\n",
    "def mrr(ranks):\n",
    "    return mean([1/item for item in ranks])\n",
    "\n",
    "print(f'MRR: {mrr(ranks)}')\n",
    "print()\n",
    "\n",
    "print(f'Checking the following values for k {ks}\\n')\n",
    "for k in ks:\n",
    "    print(f'\\tRecall at k={k}: {recall_at_k(ranks, k)}')\n",
    "    print(f'\\tPrecision at k={k}: {precision_at_k(ranks, k)}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
