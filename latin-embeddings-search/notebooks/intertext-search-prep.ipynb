{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from itertools import product\n",
    "from statistics import mean\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "TEXTS_FOLDER = \"../data/texts/process\"\n",
    "MODEL = '../models/latin_w2v_bamman_lemma300_100_1'\n",
    "VECTORS = Word2Vec.load(MODEL).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NLP tools\n",
    "\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comparison dataset\n",
    "\n",
    "comps_csv = '../data/datasets/vf_intertext_dataset_1_0.csv'\n",
    "comps = pd.read_csv(comps_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity functions\n",
    "\n",
    "def get_similarities(terms, model):\n",
    "        sims = []\n",
    "        terms = list(terms)\n",
    "        terms_ = set([y for x in terms for y in x])\n",
    "        oov = [term for term in terms_ if term not in model.vocab]\n",
    "\n",
    "        for term in terms:\n",
    "            if term[0] in oov or term[1] in oov:\n",
    "                sim = -1\n",
    "            else:\n",
    "                sim = model.similarity(term[0], term[1])\n",
    "            sims.append(sim)\n",
    "        \n",
    "        return sims\n",
    "\n",
    "# pair-aware mean\n",
    "def mean_similarities(terms, sims):\n",
    "    max_sim_index = sims.index(max(sims))\n",
    "    if max_sim_index == 0:\n",
    "        pair_index = 3\n",
    "    elif max_sim_index == 1:\n",
    "        pair_index = 2\n",
    "    elif max_sim_index == 2:\n",
    "        pair_index = 1\n",
    "    elif max_sim_index == 3:\n",
    "        pair_index = 0\n",
    "    return (sims[max_sim_index] + sims[pair_index])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text functions\n",
    "\n",
    "def preprocess(text):\n",
    "    replacer = JVReplacer()    \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    text = text.replace('ego---sed', 'egosed') # handle a tesserae text issue\n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = text.replace('â\\x80\\x94', ' ')\n",
    "    \n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def index_tess(text):\n",
    "    \n",
    "    textlines = text.strip().split('\\n')\n",
    "    # https://stackoverflow.com/a/61436083\n",
    "    def splitkeep(s, delimiter):\n",
    "        split = s.split(delimiter)\n",
    "        return [substr + delimiter for substr in split[:-1]] + [split[-1]]\n",
    "    textlines = [splitkeep(line, '>') for line in textlines if line]\n",
    "    return dict(textlines)\n",
    "\n",
    "def pp_tess(tess_dict):\n",
    "    return {k: preprocess(v) for k, v in tess_dict.items()}\n",
    "\n",
    "def text_lemmatize(lemma_pairs):\n",
    "    return \" \".join([lemma for _, lemma in lemma_pairs])\n",
    "\n",
    "def make_ref(author, work, book, line):\n",
    "    if author == 'Lucan':\n",
    "        work = ''\n",
    "    \n",
    "    if np.isnan(book):\n",
    "        book_line = f'{line}'\n",
    "    else:\n",
    "        book_line = f'{int(book)}.{int(line)}'\n",
    "        \n",
    "    ref = \" \".join(f'{author} {work} {book_line}'.split())\n",
    "    \n",
    "    return ref\n",
    "\n",
    "def make_tess_file(str):\n",
    "    #     Vergil Aeneid 1.1\n",
    "    # vergil.aeneid.part.1.tess\n",
    "    str = str.lower()\n",
    "    str = str.replace('lucan', 'lucan bellum_civile')\n",
    "    str = str.split('.')[0]\n",
    "    str = str.replace(' ', '.', 1)\n",
    "    str = str.replace(' ', '.part.', 1)\n",
    "    str += '.tess'\n",
    "    return str\n",
    "\n",
    "def make_tess_index(str):\n",
    "    str = str.replace('Lucan', 'luc.').replace('Ovid', 'ov.').replace('Statius', 'stat.').replace('Vergil', 'verg.')\n",
    "    str = str.replace('Metamorphoses', 'met.').replace('Thebaid', 'theb.').replace('Aeneid', 'aen.')\n",
    "    str = str.split('-')[0]\n",
    "    str = f'<{str}>'\n",
    "    return str\n",
    "\n",
    "def get_next_tess_index(index, n):\n",
    "    index = index.replace('>','')\n",
    "    index_base = index.split()[:-1]\n",
    "    index_ref = index.split()[-1]\n",
    "    index_ref_parts = index_ref.split('.')\n",
    "    index_ref_next = int(index_ref_parts[1])+n\n",
    "    next_index = f'{\" \".join(index_base)} {index_ref_parts[0]}.{index_ref_next}>'\n",
    "    \n",
    "    exceptions = ['<luc. 1.419>', '<luc. 7.855>', '<luc. 7.856>', '<luc. 7.857>', '<luc. 7.858>', '<luc. 7.859>', '<luc. 7.860>', '<luc. 7.861>', '<luc. 7.862>', '<luc. 7.863>', '<luc. 7.864>', '<luc. 9.414>',\n",
    "                  '<ov. met. 4.769>', \n",
    "                  '<stat. theb. 6.184>', '<stat. theb. 6.227>', '<stat. theb. 6.228>', '<stat. theb. 6.229>', '<stat. theb. 6.230>', '<stat. theb. 6.231>', '<stat. theb. 6.232>', '<stat. theb. 6.233>', '<stat. theb. 9.760>']\n",
    "    if next_index in exceptions: # Handle missing data\n",
    "        return None\n",
    "    else:\n",
    "        return next_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval/order functions\n",
    "\n",
    "def get_interval(file, index, result, orderfree):\n",
    "\n",
    "    result = replacer.replace(result.lower())\n",
    "    \n",
    "    with open(f'{TEXTS_FOLDER}/{file}') as f:\n",
    "        contents = f.read()\n",
    "        tess_dict = index_tess(contents)\n",
    "        tess_dict = pp_tess(tess_dict)\n",
    "        item = tess_dict[index]\n",
    "        \n",
    "        print(result)\n",
    "        print(item)\n",
    "\n",
    "        # Check for adjacent words in ref\n",
    "        if result in item:\n",
    "            return int(0)\n",
    "        result = result.split()\n",
    "\n",
    "        # Check for non-adjacent words in ref\n",
    "        tokens = item.split()\n",
    "        if result[0] in tokens and result[1] in tokens:\n",
    "            result_index_1 = tokens.index(result[0])\n",
    "            result_index_2 = tokens.index(result[1])\n",
    "            interval = abs(result_index_1 - result_index_2) - 1\n",
    "            return int(interval)\n",
    "\n",
    "        # Add up to 5 lines to check for words in ref\n",
    "        for i in range(1,6): # 5 line context sufficient?\n",
    "            item_extend = tess_dict[get_next_tess_index(index, i)]\n",
    "            print(item_extend)\n",
    "            if item_extend:\n",
    "                item = \" \".join([item, item_extend])\n",
    "                tokens = item.split()\n",
    "                if result[0] in tokens and result[1] in tokens:\n",
    "                    result_index_1 = tokens.index(result[0])\n",
    "                    result_index_2 = tokens.index(result[1])\n",
    "                    interval = abs(result_index_1 - result_index_2) - 1\n",
    "                    return int(interval)\n",
    "                \n",
    "    return -1\n",
    "\n",
    "\n",
    "def get_orderfree(query, result):    \n",
    "    result_b = \" \".join(result.split()[::-1])\n",
    "    comp_a = Levenshtein.distance(query, result)\n",
    "    comp_b = Levenshtein.distance(query, result_b)\n",
    "    if comp_a > comp_b:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def update_orderfree(orderfree, interval):\n",
    "    if interval:\n",
    "        orderfree = True\n",
    "    return orderfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = comps[comps['Query Phrase'].notna()]\n",
    "comps = comps[comps['Result Phrase'].notna()]\n",
    "\n",
    "comps['query_length'] = comps['Query Phrase'].apply(lambda x: len(x.split()))\n",
    "comps['result_length'] = comps['Result Phrase'].apply(lambda x: len(x.split()))\n",
    "\n",
    "comps = comps[comps['query_length'] == 2]\n",
    "comps = comps[comps['result_length'] == 2]\n",
    "\n",
    "comps['ref'] = comps.apply(lambda x: make_ref(x['Intertext: Author'], x['Intertext: Work'],x['Intertext: Book'], x['Intertext: Line Start']), axis=1)\n",
    "comps['index'] = comps.apply(lambda x: make_tess_index(x['ref']), axis=1)\n",
    "comps['file'] = comps['ref'].apply(lambda x: make_tess_file(x))\n",
    "\n",
    "comps = comps[comps['Intertext: Author'] != 'Seneca']\n",
    "comps = comps[comps['Intertext: Author'] != 'Valerius']\n",
    "\n",
    "comps['intertext_author'] = comps['ref'].apply(lambda x: x.lower().split()[0])\n",
    "comps['intertext_book'] = comps['ref'].apply(lambda x: x.split()[-1].split('.')[0])\n",
    "\n",
    "comps['Query'] = comps['Query Phrase'].apply(lambda x: replacer.replace(x.lower()))\n",
    "comps['Result'] = comps['Result Phrase'].apply(lambda x: replacer.replace(x.lower()))\n",
    "\n",
    "comps['orderfree'] = comps['Order Free']\n",
    "comps['interval'] = comps['Interval']\n",
    "\n",
    "comps['query_lemma'] = comps['Query'].apply(lambda x: \" \".join([lemma[1] for lemma in lemmatizer.lemmatize(x.lower().split())]))\n",
    "comps['target_lemma'] = comps['Result'].apply(lambda x: \" \".join([lemma[1] for lemma in lemmatizer.lemmatize(x.lower().split())]))\n",
    "comps['pairs'] = comps.apply(lambda x: tuple(product(x['query_lemma'].split(), x['target_lemma'].split())), axis=1)\n",
    "comps['similarities'] = comps.apply(lambda x: get_similarities(x['pairs'], VECTORS), axis=1)\n",
    "comps['similarity'] = comps.apply(lambda x: mean_similarities(x['pairs'], x['similarities']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude NaN intervals\n",
    "\n",
    "excluded_rows = comps[comps['interval'] == -1]\n",
    "excluded_rows.to_csv('../data/datasets/comps_excluded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export preprocessed data\n",
    "\n",
    "comps = comps[comps['interval'] != -1]\n",
    "# comps['interval'] = comps['interval'].astype(int)\n",
    "comps.to_csv('../data/datasets/comps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Comparison dataset processed...\\n\\nThere are {comps.shape[0]} rows in comparison dataset.\\n{excluded_rows.shape[0]} rows have been excluded.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}